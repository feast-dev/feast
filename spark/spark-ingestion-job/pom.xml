<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <name>Spark Ingestion job</name>
  <description>Spark job for feature ingestion</description>
  <artifactId>spark-ingestion-job</artifactId>

  <parent>
    <groupId>dev.feast</groupId>
    <artifactId>feast-parent</artifactId>
    <version>${revision}</version>
    <relativePath>../..</relativePath>
  </parent>

  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <!-- Spark 2 only runs on Java 8 -->
          <release>8</release>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-surefire-plugin</artifactId>
        <configuration>
          <argLine>-Xmx1024m -XX:MaxPermSize=256m</argLine>
        </configuration>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <executions>
          <execution>
            <goals>
              <goal>exec</goal>
            </goals>
            <configuration>
              <executable>spark-submit</executable>
              <arguments>
                <argument>--master</argument>
                <argument>local</argument>
                <argument>${project.build.directory}/${project.artifactId}-${project.version}.jar
                </argument>
              </arguments>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <!-- Use the shade plugin to remove all the provided artifacts (such as spark itself) from the jar -->
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>3.1.1</version>
        <configuration>
          <transformers>
            <transformer
                implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
              <mainClass>feast.spark.ingestion.SparkIngestion</mainClass>
            </transformer>
          </transformers>
              <!-- Relocate packages that conflict with JARs distributed in Spark. -->
              <relocations>
                <relocation>
                  <pattern>com.google.code.gson</pattern>
                  <shadedPattern>vendor.feast.com.google.code.gson</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>com.google.guava</pattern>
                  <shadedPattern>vendor.feast.com.google.guava</shadedPattern>
                </relocation>
                <relocation>
                  <pattern>com.google.protobuf</pattern>
                  <shadedPattern>vendor.feast.com.google.protobuf</shadedPattern>
                </relocation>
              </relocations>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>

  <dependencies>

    <dependency>
      <groupId>dev.feast</groupId>
      <artifactId>feast-storage-connector-redis</artifactId>
      <version>${project.version}</version>
    </dependency>

    <dependency>
      <groupId>com.google.auto.value</groupId>
      <artifactId>auto-value-annotations</artifactId>
      <version>1.6.6</version>
    </dependency>
    <dependency>
      <groupId>com.google.auto.value</groupId>
      <artifactId>auto-value</artifactId>
      <version>1.6.6</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.compat.version}</artifactId>
      <version>${spark.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-avro_${scala.compat.version}</artifactId>
      <version>${spark.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql-kafka-0-10_${scala.compat.version}</artifactId>
      <version>${spark.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>io.delta</groupId>
      <artifactId>delta-core_${scala.compat.version}</artifactId>
      <version>0.6.1</version>
      <scope>provided</scope>
    </dependency>

    <!-- To run actual Redis for ingestion integration test -->
    <dependency>
      <groupId>com.github.kstyrc</groupId>
      <artifactId>embedded-redis</artifactId>
      <scope>test</scope>
    </dependency>

    <!-- To run actual Kafka for ingestion integration test -->
    <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka_${scala.compat.version}</artifactId>
      <scope>test</scope>
    </dependency>

    <dependency>
      <groupId>io.lettuce</groupId>
      <artifactId>lettuce-core</artifactId>
    </dependency>

    <dependency>
      <groupId>org.hamcrest</groupId>
      <artifactId>hamcrest-library</artifactId>
      <scope>test</scope>
    </dependency>

    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <scope>test</scope>
    </dependency>

    <dependency>
        <groupId>dev.feast</groupId>
        <artifactId>feast-ingestion</artifactId>
        <version>${project.version}</version>
    </dependency>

  </dependencies>
</project>
