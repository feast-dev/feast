{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration Variables ---\n",
        "import os \n",
        "\n",
        "# Namespace where your resources exist\n",
        "namespace = os.environ.get(\"NAMESPACE\")\n",
        "\n",
        "fsconfigmap = \"cm-fs-data\"\n",
        "\n",
        "# Fetch token and server directly from oc CLI\n",
        "import subprocess\n",
        "\n",
        "def oc(cmd):\n",
        "    return subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip()\n",
        "\n",
        "token = oc(\"oc whoami -t\")\n",
        "server = oc(\"oc whoami --show-server\")\n",
        "\n",
        "os.environ[\"CLUSTER_TOKEN\"] = token\n",
        "os.environ[\"CLUSTER_SERVER\"] = server\n",
        "\n",
        "\n",
        "# RayCluster name\n",
        "raycluster = \"feastraytest\"\n",
        "os.environ[\"RAY_CLUSTER\"] = raycluster\n",
        "\n",
        "# Show configured values\n",
        "print(\"Configuration Variables:\")\n",
        "print(f\"  Namespace: {namespace}\")\n",
        "print(f\"  Server: {server}\")\n",
        "print(f\"  Token: {'*' * 20}\")   # hide actual token\n",
        "print(f\"  Ray Cluster: {raycluster}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https://github.com/Srihari1192/feast-rag-ray.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd feast-rag-ray/feature_repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!oc login --token=$token --server=$server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!oc create configmap $fsconfigmap --from-file=data/customer_daily_profile.parquet --from-file=data/driver_stats.parquet -n $namespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import pieces from codeflare-sdk\n",
        "from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication\n",
        "\n",
        "# Create authentication with token and server from oc\n",
        "auth = TokenAuthentication(\n",
        "    token=token,\n",
        "    server=server,\n",
        "    skip_tls=True\n",
        ")\n",
        "auth.login()\n",
        "print(\"✓ Authentication successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kubernetes.client import (\n",
        "    V1Volume,\n",
        "    V1ConfigMapVolumeSource,\n",
        "    V1VolumeMount,\n",
        ") \n",
        "\n",
        "data_volume = V1Volume(\n",
        "    name=\"data\",\n",
        "    config_map=V1ConfigMapVolumeSource(name=fsconfigmap)\n",
        ")\n",
        "\n",
        "data_mount = V1VolumeMount(\n",
        "    name=\"data\",\n",
        "    mount_path=\"/opt/app-root/src/feast-rag-ray/feature_repo/data\",\n",
        "    read_only=True\n",
        ")\n",
        "\n",
        "cluster = Cluster(ClusterConfiguration(\n",
        "    name=raycluster,\n",
        "    head_cpu_requests=1,\n",
        "    head_cpu_limits=1,\n",
        "    head_memory_requests=4,\n",
        "    head_memory_limits=4,\n",
        "    head_extended_resource_requests={'nvidia.com/gpu':0}, # For GPU enabled workloads set the head_extended_resource_requests and worker_extended_resource_requests\n",
        "    worker_extended_resource_requests={'nvidia.com/gpu':0},\n",
        "    num_workers=2,\n",
        "    worker_cpu_requests='250m',\n",
        "    worker_cpu_limits=1,\n",
        "    worker_memory_requests=4,\n",
        "    worker_memory_limits=4,\n",
        "    # image=\"\", # Optional Field \n",
        "    write_to_file=False, # When enabled Ray Cluster yaml files are written to /HOME/.codeflare/resources\n",
        "    local_queue=\"fs-user-queue\", # Specify the local queue manually\n",
        "        # ⭐ Best method: Use secretKeyRef to expose AWS credentials safely\n",
        "    volumes=[data_volume],\n",
        "    volume_mounts=[data_mount],\n",
        "    \n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster.apply()\n",
        "# cluster.wait_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "MAX_WAIT = 180        # 3 minutes\n",
        "INTERVAL = 5          # check every 5 seconds\n",
        "elapsed = 0\n",
        "\n",
        "print(\"⏳ Waiting up to 3 minutes for RayCluster to be READY...\\n\")\n",
        "\n",
        "while elapsed < MAX_WAIT:\n",
        "    details = cluster.details()\n",
        "    status = details.status.value\n",
        "\n",
        "    print(details)\n",
        "    print(\"Cluster Status:\", status)\n",
        "\n",
        "    if status == \"ready\":\n",
        "        print(\"✅ RayCluster is READY!\")\n",
        "        break\n",
        "    \n",
        "    print(f\"⏳ RayCluster is NOT ready yet: {status} ... checking again in {INTERVAL}s\\n\")\n",
        "    time.sleep(INTERVAL)\n",
        "    elapsed += INTERVAL\n",
        "\n",
        "else:\n",
        "    print(\"❌ Timeout: RayCluster did NOT become READY within 3 minutes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! feast apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "from feast import FeatureStore\n",
        "\n",
        "# Add feature repo to PYTHONPATH\n",
        "repo_path = Path(\".\")\n",
        "sys.path.append(str(repo_path))\n",
        "\n",
        "# Initialize Feature Store\n",
        "print(\"Initializing Feast with Ray configuration...\")\n",
        "store = FeatureStore(repo_path=\".\")\n",
        "\n",
        "# Assertions: Verify store is initialized correctly\n",
        "assert store is not None, \"FeatureStore should be initialized\"\n",
        "assert store.config is not None, \"Store config should be available\"\n",
        "assert store.config.offline_store is not None, \"Offline store should be configured\"\n",
        "\n",
        "print(f\"✓ Offline store: {store.config.offline_store.type}\")\n",
        "if hasattr(store.config, \"batch_engine\") and store.config.batch_engine:\n",
        "    print(f\"✓ Compute engine: {store.config.batch_engine.type}\")\n",
        "    # Assertion: Verify batch engine is configured if present\n",
        "    assert store.config.batch_engine.type is not None, \"Batch engine type should be set\"\n",
        "else:\n",
        "    print(\"⚠ No compute engine configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Entity DataFrame\n",
        "\n",
        "Create an entity DataFrame for historical feature retrieval with point-in-time timestamps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "# --- Create time window ---\n",
        "end_date = datetime.now().replace(microsecond=0, second=0, minute=0)\n",
        "start_date = end_date - timedelta(days=2)\n",
        "\n",
        "\n",
        "entity_df = pd.DataFrame(\n",
        "    {\n",
        "        \"driver_id\": [1001, 1002, 1003],\n",
        "        \"customer_id\": [2001, 2002, 2003],\n",
        "        \"event_timestamp\": [\n",
        "            pd.Timestamp(end_date - timedelta(hours=24), tz=\"UTC\"),\n",
        "            pd.Timestamp(end_date - timedelta(hours=12), tz=\"UTC\"),\n",
        "            pd.Timestamp(end_date - timedelta(hours=6), tz=\"UTC\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Assertions: Verify entity DataFrame is created correctly\n",
        "assert len(entity_df) == 3, f\"Expected 3 rows, got {len(entity_df)}\"\n",
        "assert \"driver_id\" in entity_df.columns, \"driver_id column should be present\"\n",
        "assert \"customer_id\" in entity_df.columns, \"customer_id column should be present\"\n",
        "assert \"event_timestamp\" in entity_df.columns, \"event_timestamp column should be present\"\n",
        "assert all(entity_df[\"driver_id\"].isin([1001, 1002, 1003])), \"driver_id values should match expected\"\n",
        "assert all(entity_df[\"customer_id\"].isin([2001, 2002, 2003])), \"customer_id values should match expected\"\n",
        "assert entity_df[\"event_timestamp\"].notna().all(), \"All event_timestamp values should be non-null\"\n",
        "\n",
        "print(f\"✓ Created entity DataFrame with {len(entity_df)} rows\")\n",
        "print(f\"✓ Time range: {start_date} to {end_date}\")\n",
        "print(\"\\nEntity DataFrame:\")\n",
        "print(entity_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Retrieve Historical Features\n",
        "\n",
        "Retrieve historical features using Ray compute engine for distributed point-in-time joins.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Retrieve Historical Features\n",
        "print(\"Retrieving historical features with Ray compute engine...\")\n",
        "print(\"(This demonstrates distributed point-in-time joins)\")\n",
        "\n",
        "try:\n",
        "    # Get historical features - this uses Ray compute engine for distributed processing\n",
        "    historical_features = store.get_historical_features(\n",
        "        entity_df=entity_df,\n",
        "        features=[\n",
        "            \"driver_hourly_stats:conv_rate\",\n",
        "            \"driver_hourly_stats:acc_rate\",\n",
        "            \"driver_hourly_stats:avg_daily_trips\",\n",
        "            \"customer_daily_profile:current_balance\",\n",
        "            \"customer_daily_profile:avg_passenger_count\",\n",
        "            \"customer_daily_profile:lifetime_trip_count\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Convert to DataFrame - Ray processes this efficiently\n",
        "    historical_df = historical_features.to_df()\n",
        "    \n",
        "    # Assertions: Verify historical features are retrieved correctly\n",
        "    assert historical_df is not None, \"Historical features DataFrame should not be None\"\n",
        "    assert len(historical_df) > 0, \"Should retrieve at least one row of historical features\"\n",
        "    assert \"driver_id\" in historical_df.columns, \"driver_id should be in the result\"\n",
        "    assert \"customer_id\" in historical_df.columns, \"customer_id should be in the result\"\n",
        "    \n",
        "    # Verify expected feature columns are present (some may be None if data doesn't exist)\n",
        "    expected_features = [\n",
        "        \"conv_rate\", \"acc_rate\", \"avg_daily_trips\",\n",
        "        \"current_balance\", \"avg_passenger_count\", \"lifetime_trip_count\"\n",
        "    ]\n",
        "    feature_columns = [col for col in historical_df.columns if col in expected_features]\n",
        "    assert len(feature_columns) > 0, f\"Should have at least one feature column, got: {historical_df.columns.tolist()}\"\n",
        "    \n",
        "    print(f\"✓ Retrieved {len(historical_df)} historical feature rows\")\n",
        "    print(f\"✓ Features: {list(historical_df.columns)}\")\n",
        "    \n",
        "    # Display the results\n",
        "    print(\"\\nHistorical Features DataFrame:\")\n",
        "    display(historical_df.head(10))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Historical features retrieval failed: {e}\")\n",
        "    print(\"This might be due to missing Ray dependencies or data\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test On-Demand Feature Transformations\n",
        "\n",
        "Demonstrate on-demand feature transformations that are computed at request time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Test On-Demand Features\n",
        "print(\"Testing on-demand feature transformations...\")\n",
        "\n",
        "try:\n",
        "    # Get features including on-demand transformations\n",
        "    features_with_odfv = store.get_historical_features(\n",
        "        entity_df=entity_df.head(1),\n",
        "        features=[\n",
        "            \"driver_hourly_stats:conv_rate\",\n",
        "            \"driver_hourly_stats:acc_rate\",\n",
        "            \"driver_hourly_stats:avg_daily_trips\",\n",
        "            \"driver_activity_v2:conv_rate_plus_acc_rate\",\n",
        "            \"driver_activity_v2:trips_per_day_normalized\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    odfv_df = features_with_odfv.to_df()\n",
        "    \n",
        "    # Assertions: Verify on-demand features are computed correctly\n",
        "    assert odfv_df is not None, \"On-demand features DataFrame should not be None\"\n",
        "    assert len(odfv_df) > 0, \"Should retrieve at least one row with on-demand features\"\n",
        "    assert \"driver_id\" in odfv_df.columns, \"driver_id should be in the result\"\n",
        "    \n",
        "    # Verify on-demand feature columns if they exist\n",
        "    if \"conv_rate_plus_acc_rate\" in odfv_df.columns:\n",
        "        # Assertion: Verify the on-demand feature is computed\n",
        "        assert odfv_df[\"conv_rate_plus_acc_rate\"].notna().any(), \"conv_rate_plus_acc_rate should have non-null values\"\n",
        "        print(\"✓ On-demand feature 'conv_rate_plus_acc_rate' is computed\")\n",
        "    \n",
        "    if \"trips_per_day_normalized\" in odfv_df.columns:\n",
        "        assert odfv_df[\"trips_per_day_normalized\"].notna().any(), \"trips_per_day_normalized should have non-null values\"\n",
        "        print(\"✓ On-demand feature 'trips_per_day_normalized' is computed\")\n",
        "    \n",
        "    print(f\"✓ Retrieved {len(odfv_df)} rows with on-demand transformations\")\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\nFeatures with On-Demand Transformations:\")\n",
        "    display(odfv_df)\n",
        "    \n",
        "    # Show specific transformed features\n",
        "    if \"conv_rate_plus_acc_rate\" in odfv_df.columns:\n",
        "        print(\"\\nSample with on-demand features:\")\n",
        "        display(\n",
        "            odfv_df[[\"driver_id\", \"conv_rate\", \"acc_rate\", \"conv_rate_plus_acc_rate\"]]\n",
        "        )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ On-demand features failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Materialize Features to Online Store\n",
        "\n",
        "Materialize features to the online store using Ray compute engine for efficient batch processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import timezone\n",
        "print(\"Materializing features to online store...\")\n",
        "store.materialize(\n",
        "\tstart_date=datetime(2025, 1, 1, tzinfo=timezone.utc),\n",
        "\tend_date=end_date,\n",
        ")\n",
        "\n",
        "# Minimal output assertion: materialization succeeded if no exception\n",
        "assert True, \"Materialization completed successfully\"\n",
        "print(\"✓ Initial materialization successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Online Feature Serving\n",
        "\n",
        "Retrieve features from the online store for low-latency serving.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Test Online Feature Serving\n",
        "print(\"Testing online feature serving...\")\n",
        "\n",
        "try:\n",
        "    entity_rows = [\n",
        "        {\"driver_id\": 1001, \"customer_id\": 2001},\n",
        "        {\"driver_id\": 1002, \"customer_id\": 2002},\n",
        "    ]\n",
        "    \n",
        "    # Assertion: Verify entity rows are valid\n",
        "    assert len(entity_rows) == 2, \"Should have 2 entity rows\"\n",
        "    assert all(\"driver_id\" in row for row in entity_rows), \"All entity rows should have driver_id\"\n",
        "    assert all(\"customer_id\" in row for row in entity_rows), \"All entity rows should have customer_id\"\n",
        "    \n",
        "    online_features = store.get_online_features(\n",
        "        features=[\n",
        "            \"driver_hourly_stats:conv_rate\",\n",
        "            \"driver_hourly_stats:acc_rate\",\n",
        "            \"customer_daily_profile:current_balance\",\n",
        "        ],\n",
        "        entity_rows=entity_rows,\n",
        "    )\n",
        "\n",
        "    online_df = online_features.to_df()\n",
        "    \n",
        "    # Assertions: Verify online features are retrieved correctly\n",
        "    assert online_df is not None, \"Online features DataFrame should not be None\"\n",
        "    assert len(online_df) == len(entity_rows), f\"Should retrieve {len(entity_rows)} rows, got {len(online_df)}\"\n",
        "    assert \"driver_id\" in online_df.columns, \"driver_id should be in the result\"\n",
        "    assert \"customer_id\" in online_df.columns, \"customer_id should be in the result\"\n",
        "    \n",
        "    # Verify expected feature columns are present\n",
        "    expected_features = [\"conv_rate\", \"acc_rate\", \"current_balance\"]\n",
        "    feature_columns = [col for col in online_df.columns if col in expected_features]\n",
        "    assert len(feature_columns) > 0, f\"Should have at least one feature column, got: {online_df.columns.tolist()}\"\n",
        "    \n",
        "    # Verify entity IDs match\n",
        "    assert all(online_df[\"driver_id\"].isin([1001, 1002])), \"driver_id values should match entity rows\"\n",
        "    assert all(online_df[\"customer_id\"].isin([2001, 2002])), \"customer_id values should match entity rows\"\n",
        "    \n",
        "    print(f\"✓ Retrieved {len(online_df)} online feature rows\")\n",
        "    print(f\"✓ Features retrieved: {feature_columns}\")\n",
        "    \n",
        "    print(\"\\nOnline Features DataFrame:\")\n",
        "    display(online_df)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Online serving failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster.down()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
