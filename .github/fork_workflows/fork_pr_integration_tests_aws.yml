name: fork-pr-integration-tests-aws

on: [pull_request]

jobs:
  integration-test-python:
    if: github.repository == 'your github repo' # swap here with your project id
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        python-version: [ "3.11" ]
        os: [ ubuntu-latest ]
    env:
      OS: ${{ matrix.os }}
      PYTHON: ${{ matrix.python-version }}
    services:
      redis:
        image: redis
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4
        with:
          # pull_request_target runs the workflow in the context of the base repo
          # as such actions/checkout needs to be explicit configured to retrieve
          # code from the PR.
          ref: refs/pull/${{ github.event.pull_request.number }}/merge
          submodules: recursive
      - name: Setup Python
        uses: actions/setup-python@v5
        id: setup-python
        with:
          python-version: ${{ matrix.python-version }}
          architecture: x64
      - name: Setup Go
        id: setup-go
        uses: actions/setup-go@v2
        with:
          go-version: 1.18.0
      - name: Set up AWS SDK
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-west-2
      - name: Use AWS CLI
        run: aws sts get-caller-identity
      - name: Install the latest version of uv and set the python version
        uses: astral-sh/setup-uv@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Get uv cache dir
        id: uv-cache
        run: |
          echo "::set-output name=dir::$(uv cache dir)"
      - name: uv cache
        uses: actions/cache@v4
        with:
          path: ${{ steps.uv-cache.outputs.dir }}
          key: ${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-uv-${{ hashFiles(format('**/py{0}-ci-requirements.txt', env.PYTHON)) }}
      - name: Install apache-arrow on ubuntu
        if: matrix.os == 'ubuntu-latest'
        run: |
            sudo apt update
            sudo apt install -y -V ca-certificates lsb-release wget
            wget https://apache.jfrog.io/artifactory/arrow/$(lsb_release --id --short | tr 'A-Z' 'a-z')/apache-arrow-apt-source-latest-$(lsb_release --codename --short).deb
            sudo apt install -y -V ./apache-arrow-apt-source-latest-$(lsb_release --codename --short).deb
            sudo apt update
            sudo apt install -y -V libarrow-dev
      - name: Install dependencies
        run: make install-python-dependencies-ci
      - name: Setup Redis Cluster
        run: |
          docker pull vishnunair/docker-redis-cluster:latest
          docker run -d -p 6001:6379 -p 6002:6380 -p 6003:6381 -p 6004:6382 -p 6005:6383 -p 6006:6384 --name redis-cluster vishnunair/docker-redis-cluster
      - name: Test python
        if: ${{ always() }}  # this will guarantee that step won't be canceled and resources won't leak
        run: |
          pytest -n 8 --cov=./ --cov-report=xml --color=yes sdk/python/tests --integration --durations=5 --timeout=1200 --timeout_method=thread -k "aws and not Snowflake and not BigQuery and not minio_registry"
          pytest -n 8 --cov=./ --cov-report=xml --color=yes sdk/python/tests --integration --durations=5 --timeout=1200 --timeout_method=thread -k "File and not Snowflake and not BigQuery and not minio_registry"
          pytest -n 8 --cov=./ --cov-report=xml --color=yes sdk/python/tests --integration --durations=5 --timeout=1200 --timeout_method=thread -k "dynamo and not Snowflake and not BigQuery and not minio_registry"
          pytest -n 8 --cov=./ --cov-report=xml --color=yes sdk/python/tests --integration --durations=5 --timeout=1200 --timeout_method=thread -k "Redshift and not Snowflake and not BigQuery and not minio_registry"
