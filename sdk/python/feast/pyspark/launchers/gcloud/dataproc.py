import os
import uuid
from functools import partial
from typing import Any, Callable, Dict, List
from urllib.parse import urlparse

from google.api_core.operation import Operation
from google.cloud import dataproc_v1
from google.cloud.dataproc_v1 import JobStatus

from feast.pyspark.abc import (
    BatchIngestionJob,
    BatchIngestionJobParameters,
    JobLauncher,
    RetrievalJob,
    RetrievalJobParameters,
    SparkJob,
    SparkJobFailure,
    SparkJobParameters,
    SparkJobStatus,
    StreamIngestionJob,
    StreamIngestionJobParameters,
)
from feast.staging.storage_client import get_staging_client


class DataprocJobMixin:
    def __init__(self, operation: Operation, cancel_fn: Callable[[], None]):
        """
        :param operation: (google.api.core.operation.Operation): A Future for the spark job result,
                returned by the dataproc client.
        """
        self._operation = operation
        self._cancel_fn = cancel_fn

    def get_id(self) -> str:
        return self._operation.metadata.job_id

    def get_status(self) -> SparkJobStatus:
        self._operation._refresh_and_update()

        status = self._operation.metadata.status
        if status.state == JobStatus.State.ERROR:
            return SparkJobStatus.FAILED
        elif status.state == JobStatus.State.RUNNING:
            return SparkJobStatus.IN_PROGRESS
        elif status.state in (
            JobStatus.State.PENDING,
            JobStatus.State.SETUP_DONE,
            JobStatus.State.STATE_UNSPECIFIED,
        ):
            return SparkJobStatus.STARTING

        return SparkJobStatus.COMPLETED

    def cancel(self):
        self._cancel_fn()


class DataprocRetrievalJob(DataprocJobMixin, RetrievalJob):
    """
    Historical feature retrieval job result for a Dataproc cluster
    """

    def __init__(
        self, operation: Operation, cancel_fn: Callable[[], None], output_file_uri: str
    ):
        """
        This is the returned historical feature retrieval job result for DataprocClusterLauncher.

        Args:
            output_file_uri (str): Uri to the historical feature retrieval job output file.
        """
        super().__init__(operation, cancel_fn)
        self._output_file_uri = output_file_uri

    def get_output_file_uri(self, timeout_sec=None, block=True):
        if not block:
            return self._output_file_uri

        try:
            self._operation.result(timeout_sec)
        except Exception as err:
            raise SparkJobFailure(err)
        return self._output_file_uri


class DataprocBatchIngestionJob(DataprocJobMixin, BatchIngestionJob):
    """
    Batch Ingestion job result for a Dataproc cluster
    """


class DataprocStreamingIngestionJob(DataprocJobMixin, StreamIngestionJob):
    """
    Streaming Ingestion job result for a Dataproc cluster
    """


class DataprocClusterLauncher(JobLauncher):
    """
    Submits jobs to an existing Dataproc cluster. Depends on google-cloud-dataproc and
    google-cloud-storage, which are optional dependencies that the user has to installed in
    addition to the Feast SDK.
    """

    def __init__(
        self, cluster_name: str, staging_location: str, region: str, project_id: str,
    ):
        """
        Initialize a dataproc job controller client, used internally for job submission and result
        retrieval.

        Args:
            cluster_name (str):
                Dataproc cluster name.
            staging_location (str):
                GCS directory for the storage of files generated by the launcher, such as the pyspark scripts.
            region (str):
                Dataproc cluster region.
            project_id (str:
                GCP project id for the dataproc cluster.
        """

        self.cluster_name = cluster_name

        scheme, self.staging_bucket, self.remote_path, _, _, _ = urlparse(
            staging_location
        )
        if scheme != "gs":
            raise ValueError(
                "Only GCS staging location is supported for DataprocLauncher."
            )
        self.project_id = project_id
        self.region = region
        self.job_client = dataproc_v1.JobControllerClient(
            client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"}
        )

    def _stage_files(self, pyspark_script: str, job_id: str) -> str:
        staging_client = get_staging_client("gs")
        blob_path = os.path.join(
            self.remote_path, job_id, os.path.basename(pyspark_script),
        )
        staging_client.upload_file(pyspark_script, self.staging_bucket, blob_path)

        return f"gs://{self.staging_bucket}/{blob_path}"

    def dataproc_submit(self, job_params: SparkJobParameters) -> Operation:
        local_job_id = str(uuid.uuid4())
        main_file_uri = self._stage_files(job_params.get_main_file_path(), local_job_id)
        job_config: Dict[str, Any] = {
            "reference": {"job_id": local_job_id},
            "placement": {"cluster_name": self.cluster_name},
        }
        if job_params.get_class_name():
            job_config.update(
                {
                    "spark_job": {
                        "jar_file_uris": [main_file_uri],
                        "main_class": job_params.get_class_name(),
                        "args": job_params.get_arguments(),
                    }
                }
            )
        else:
            job_config.update(
                {
                    "pyspark_job": {
                        "main_python_file_uri": main_file_uri,
                        "args": job_params.get_arguments(),
                    }
                }
            )
        return self.job_client.submit_job_as_operation(
            request={
                "project_id": self.project_id,
                "region": self.region,
                "job": job_config,
            }
        )

    def dataproc_cancel(self, job_id):
        self.job_client.cancel_job(
            project_id=self.project_id, region=self.region, job_id=job_id
        )

    def historical_feature_retrieval(
        self, job_params: RetrievalJobParameters
    ) -> RetrievalJob:
        operation = self.dataproc_submit(job_params)
        cancel_fn = partial(self.dataproc_cancel, operation.metadata.job_id)
        return DataprocRetrievalJob(
            operation, cancel_fn, job_params.get_destination_path()
        )

    def offline_to_online_ingestion(
        self, ingestion_job_params: BatchIngestionJobParameters
    ) -> BatchIngestionJob:
        operation = self.dataproc_submit(ingestion_job_params)
        cancel_fn = partial(self.dataproc_cancel, operation.metadata.job_id)
        return DataprocBatchIngestionJob(operation, cancel_fn)

    def start_stream_to_online_ingestion(
        self, ingestion_job_params: StreamIngestionJobParameters
    ) -> StreamIngestionJob:
        operation = self.dataproc_submit(ingestion_job_params)
        cancel_fn = partial(self.dataproc_cancel, operation.metadata.job_id)
        return DataprocStreamingIngestionJob(operation, cancel_fn)

    def stage_dataframe(self, df, event_timestamp_column: str):
        raise NotImplementedError

    def get_job_by_id(self, job_id: str) -> SparkJob:
        raise NotImplementedError

    def list_jobs(self, include_terminated: bool) -> List[SparkJob]:
        raise NotImplementedError
