import json
import os
from typing import Dict, List
from urllib.parse import urlparse

from google.cloud import dataproc_v1, storage

from feast.pyspark.abc import JobLauncher, RetrievalJob, SparkJobFailure


class DataprocRetrievalJob(RetrievalJob):
    """
    Historical feature retrieval job result for a Dataproc cluster
    """

    def __init__(self, job_id, operation, output_file_uri):
        """
        This is the returned historical feature retrieval job result for DataprocClusterLauncher.

        Args:
            job_id (str): Historical feature retrieval job id.
            operation (google.api.core.operation.Operation): A Future for the spark job result,
                returned by the dataproc client.
            output_file_uri (str): Uri to the historical feature retrieval job output file.
        """
        self.job_id = job_id
        self._operation = operation
        self._output_file_uri = output_file_uri

    def get_id(self) -> str:
        return self.job_id

    def get_output_file_uri(self, timeout_sec=None):
        try:
            self._operation.result(timeout_sec)
        except Exception as err:
            raise SparkJobFailure(err)
        return self._output_file_uri


class DataprocClusterLauncher(JobLauncher):
    """
    Submits jobs to an existing Dataproc cluster. Depends on google-cloud-dataproc and
    google-cloud-storage, which are optional dependencies that the user has to installed in
    addition to the Feast SDK.
    """

    def __init__(
        self, cluster_name: str, staging_location: str, region: str, project_id: str,
    ):
        """
        Initialize a dataproc job controller client, used internally for job submission and result
        retrieval.

        Args:
            cluster_name (str):
                Dataproc cluster name.
            staging_location (str):
                GCS directory for the storage of files generated by the launcher, such as the pyspark scripts.
            region (str):
                Dataproc cluster region.
            project_id (str:
                GCP project id for the dataproc cluster.
        """

        self.cluster_name = cluster_name

        scheme, self.staging_bucket, self.remote_path, _, _, _ = urlparse(
            staging_location
        )
        if scheme != "gs":
            raise ValueError(
                "Only GCS staging location is supported for DataprocLauncher."
            )
        self.project_id = project_id
        self.region = region
        self.job_client = dataproc_v1.JobControllerClient(
            client_options={"api_endpoint": f"{region}-dataproc.googleapis.com:443"}
        )

    def _stage_files(self, pyspark_script: str, job_id: str) -> str:
        client = storage.Client()
        bucket = client.get_bucket(self.staging_bucket)
        blob_path = os.path.join(
            self.remote_path, job_id, os.path.basename(pyspark_script),
        )
        blob = bucket.blob(blob_path)
        blob.upload_from_filename(pyspark_script)

        return f"gs://{self.staging_bucket}/{blob_path}"

    def historical_feature_retrieval(
        self,
        pyspark_script: str,
        entity_source_conf: Dict,
        feature_tables_sources_conf: List[Dict],
        feature_tables_conf: List[Dict],
        destination_conf: Dict,
        job_id: str,
        **kwargs,
    ) -> RetrievalJob:

        pyspark_gcs = self._stage_files(pyspark_script, job_id)
        job = {
            "reference": {"job_id": job_id},
            "placement": {"cluster_name": self.cluster_name},
            "pyspark_job": {
                "main_python_file_uri": pyspark_gcs,
                "args": [
                    "--feature-tables",
                    json.dumps(feature_tables_conf),
                    "--feature-tables-sources",
                    json.dumps(feature_tables_sources_conf),
                    "--entity-source",
                    json.dumps(entity_source_conf),
                    "--destination",
                    json.dumps(destination_conf),
                ],
            },
        }
        operation = self.job_client.submit_job_as_operation(
            request={"project_id": self.project_id, "region": self.region, "job": job}
        )
        output_file = destination_conf["path"]
        return DataprocRetrievalJob(job_id, operation, output_file)
