//
// * Copyright 2020 The Feast Authors
// *
// * Licensed under the Apache License, Version 2.0 (the "License");
// * you may not use this file except in compliance with the License.
// * You may obtain a copy of the License at
// *
// *     https://www.apache.org/licenses/LICENSE-2.0
// *
// * Unless required by applicable law or agreed to in writing, software
// * distributed under the License is distributed on an "AS IS" BASIS,
// * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// * See the License for the specific language governing permissions and
// * limitations under the License.
//

syntax = "proto3";
package feast.core;

option java_package = "feast.proto.core";
option java_outer_classname = "RunnerProto";
option go_package = "github.com/feast-dev/feast/sdk/go/protos/feast/core";

message DirectRunnerConfigOptions {
    /**
     * Controls the amount of target parallelism the DirectRunner will use.
     * Defaults to the greater of the number of available processors and 3. Must be a value
     * greater than zero.
     */
    int32 targetParallelism = 1;

    /* BigQuery table specification, e.g. PROJECT_ID:DATASET_ID.PROJECT_ID */
    string deadLetterTableSpec = 2;

    // A pipeline level default location for storing temporary files.
    // Support Google Cloud Storage locations or local path
    string tempLocation = 3;
}

message DataflowRunnerConfigOptions {
    /* Project id to use when launching jobs. */
    string project = 1;

    /* The Google Compute Engine region for creating Dataflow jobs. */
    string region = 2;

    /* GCP availability zone for operations. */
    string workerZone = 3;

    /* Run the job as a specific service account, instead of the default GCE robot. */
    string serviceAccount = 4;

    /* GCE network for launching workers. */
    string network = 5;

    /* GCE subnetwork for launching workers. e.g. regions/asia-east1/subnetworks/mysubnetwork */
    string subnetwork = 6;

    /*  Machine type to create Dataflow worker VMs as. */
    string workerMachineType = 7;

    /* The autoscaling algorithm to use for the workerpool. */
    string autoscalingAlgorithm = 8;

    /* Specifies whether worker pools should be started with public IP addresses. */
    bool usePublicIps = 9;

    // A pipeline level default location for storing temporary files.  Support Google Cloud Storage locations,
    // e.g. gs://bucket/object
    string tempLocation = 10;

    /* The maximum number of workers to use for the workerpool. */
    int32 maxNumWorkers = 11;

    /* BigQuery table specification, e.g. PROJECT_ID:DATASET_ID.PROJECT_ID */
    string deadLetterTableSpec = 12;

    /* Labels to apply to the dataflow job */
    map<string, string> labels = 13;

    /* Disk size to use on each remote Compute Engine worker instance */
    int32 diskSizeGb = 14;

    /* Run job on Dataflow Streaming Engine instead of creating worker VMs */
    bool enableStreamingEngine = 15;

    /* Type of persistent disk to be used by workers */
    string workerDiskType = 16;
}