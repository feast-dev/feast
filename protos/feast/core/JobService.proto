//
// Copyright 2018 The Feast Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

syntax = "proto3";
package feast.core;

option go_package = "github.com/feast-dev/feast/sdk/go/protos/feast/core";
option java_outer_classname = "JobServiceProto";
option java_package = "feast.proto.core";

import "google/protobuf/timestamp.proto";
import "feast/core/DataSource.proto";


service JobService {
    // Start job to ingest data from offline store into online store
    rpc StartOfflineToOnlineIngestionJob (StartOfflineToOnlineIngestionJobRequest) returns (StartOfflineToOnlineIngestionJobResponse);

    // Produce a training dataset, return a job id that will provide a file reference
    rpc GetHistoricalFeatures (GetHistoricalFeaturesRequest) returns (GetHistoricalFeaturesResponse);

    // Start job to ingest data from stream into online store
    rpc StartStreamToOnlineIngestionJob (StartStreamToOnlineIngestionJobRequest) returns (StartStreamToOnlineIngestionJobResponse);

    // List all types of jobs
    rpc ListJobs (ListJobsRequest) returns (ListJobsResponse);

    // Cancel a single job
    rpc CancelJob (CancelJobRequest) returns (CancelJobResponse);

    // Get details of a single job
    rpc GetJob (GetJobRequest) returns (GetJobResponse);
}


enum JobType {
	INVALID_JOB = 0;
	BATCH_INGESTION_JOB = 1;
	STREAM_INGESTION_JOB = 2;
	RETRIEVAL_JOB = 4;
}

enum JobStatus {
	JOB_STATUS_INVALID = 0;
       // The Job has be registered and waiting to get scheduled to run
	JOB_STATUS_PENDING = 1;
       // The Job is currently processing its task
	JOB_STATUS_RUNNING = 2;
       // The Job has successfully completed its task
	JOB_STATUS_DONE = 3;
       // The Job has encountered an error while processing its task
	JOB_STATUS_ERROR = 4;
}

message Job {
  // Identifier of the Job
  string id = 1;
  // Type of the Job
  JobType type = 2;
  // Current job status
  JobStatus status = 3;
  // Deterministic hash of the Job
  string hash = 4;
  // Start time of the Job
  google.protobuf.Timestamp start_time = 5;

  message RetrievalJobMeta {
    string output_location = 1;
  }

  message OfflineToOnlineMeta {
      string table_name = 1;
  }

  message StreamToOnlineMeta {
      string table_name = 1;
  }

  // JobType specific metadata on the job
  oneof meta {
    RetrievalJobMeta retrieval = 6;
    OfflineToOnlineMeta batch_ingestion = 7;
    StreamToOnlineMeta stream_ingestion = 8;
  }

  // Path to Spark job logs, if available
  string log_uri = 9;
}

// Ingest data from offline store into online store
message StartOfflineToOnlineIngestionJobRequest {
    // Feature table to ingest
    string project = 1;
    string table_name = 2;

    // Start of time range for source data from offline store
    google.protobuf.Timestamp start_date = 3;

    // End of time range for source data from offline store
    google.protobuf.Timestamp end_date = 4;
}

message StartOfflineToOnlineIngestionJobResponse {
  // Job ID assigned by Feast
  string id = 1;

  // Job start time
  google.protobuf.Timestamp job_start_time = 2;

  // Feature table associated with the job
  string table_name = 3;

  // Path to Spark job logs, if available
  string log_uri = 4;
}

message GetHistoricalFeaturesRequest {
  // List of feature references that are being retrieved
  repeated string feature_refs = 1;

  // Batch DataSource that can be used to obtain entity values for historical retrieval.
  // For each entity value, a feature value will be retrieved for that value/timestamp
  // Only 'BATCH_*' source types are supported.
  // Currently only BATCH_FILE source type is supported.
  DataSource entity_source = 2;

  // Optional field to specify project name override. If specified, uses the
  // given project for retrieval. Overrides the projects specified in
  // Feature References if both are specified.
  string project = 3;

  // Specifies the path in a bucket to write the exported feature data files
  // Export to AWS S3 - s3://path/to/features
  // Export to GCP GCS -  gs://path/to/features
  string output_location = 4;

  // Specify format name for output, eg. parquet
  string output_format = 5;
}

message GetHistoricalFeaturesResponse {
  // Export Job with ID assigned by Feast
  string id = 1;

  // Uri to the join result output file
  string output_file_uri = 2;

  // Job start time
  google.protobuf.Timestamp job_start_time = 3;

  // Path to Spark job logs, if available
  string log_uri = 4;

}

message StartStreamToOnlineIngestionJobRequest {
    // Feature table to ingest
    string project = 1;
    string table_name = 2;
}

message StartStreamToOnlineIngestionJobResponse {
  // Job ID assigned by Feast
  string id = 1;

  // Job start time
  google.protobuf.Timestamp job_start_time = 2;

  // Feature table associated with the job
  string table_name = 3;

  // Path to Spark job logs, if available
  string log_uri = 4;
}

message ListJobsRequest {
  bool include_terminated = 1;
  string table_name = 2;
}

message ListJobsResponse {
  repeated Job jobs = 1;
}

message GetJobRequest {
  string job_id = 1;
}

message GetJobResponse {
  Job job = 1;
}

message CancelJobRequest{
  string job_id = 1;
}

message CancelJobResponse {}