{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building streaming features with Feast, Spark, and Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "This notebook explores how data scientists and engineers can build streaming features in Feast. It builds off of the first [module](https://github.com/feast-dev/feast-workshop/tree/main/module_1) from the Feast workshop. The workshop module is not a prerequisite, but is recommended if you are not already familiar with the Push API in Feast.\n",
    "\n",
    "A typical pattern that we see is that data scientists use Feast to define, use, and share features, which greatly improves their productivity, while ML engineers use Feast to ensure that all the features are available for both training and serving. This notebook will first show how a data scientist can define and use stream features in Feast, and then how an ML engineer can use the Push API to ensure that those streaming features are available in production.\n",
    "\n",
    "All the necessary resources to run this notebook are in `kafka_spark_demo`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feature repository\n",
    "We first run `feast apply` to register the data sources and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/specifiers.py:273: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  warnings.warn(\n",
      "/Users/felixwang/feast/env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py:111: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  warnings.warn(\n",
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "Created entity \u001b[1m\u001b[32mdriver\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mcustomer\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mcustomer_stats\u001b[0m\n",
      "Created stream feature view \u001b[1m\u001b[32mdriver_hourly_stats_stream\u001b[0m\n",
      "\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mcustomer_stats\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spin up Kafka + Redis + Feast services\n",
    "\n",
    "We then use Docker Compose to spin up the services we need by running `docker-compose up` from the `kafka_spark_demo` directory.\n",
    "* This leverages a script (in `kafka_demo/`) that creates a topic, reads from `feature_repo/data/driver_stats_stream.parquet`, generates newer timestamps, and emits them to the topic.\n",
    "* This also deploys an instance of Redis.\n",
    "\n",
    "```\n",
    "$ docker-compose up\n",
    "Creating network \"kafka_spark_demo_default\" with the default driver\n",
    "Creating redis     ... done\n",
    "Creating zookeeper ... done\n",
    "Creating broker    ... done\n",
    "Creating kafka_events ... done\n",
    "Attaching to zookeeper, redis, broker, kafka_events\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore existing feature views\n",
    "\n",
    "Let's assume the role of a data scientist who wants to train a model to determine which customers and drivers should be matched together. We start by exploring the existing feature views, as another data scientist might have already defined a useful feature view.\n",
    "\n",
    "If we inspect `features.py`, we'll see that a `customer_stats_view` feature view already exists. Let's take a look at the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id                  event_timestamp  current_balance  \\\n",
      "0         5001        2021-04-12 10:59:42+00:00         0.174109   \n",
      "1         5003        2021-04-12 16:40:26+00:00         0.735872   \n",
      "2         5004        2021-04-12 15:01:12+00:00         0.885541   \n",
      "3         5002        2021-04-12 08:12:10+00:00         0.922777   \n",
      "4         5001 2022-06-17 11:32:46.619010+00:00         0.544859   \n",
      "\n",
      "   avg_passenger_count  lifetime_trip_count  \n",
      "0             0.384933                   14  \n",
      "1             0.542926                  616  \n",
      "2             0.774241                  129  \n",
      "3             0.167704                  844  \n",
      "4             0.087846                  240  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"customer_id\": [5001, 5002, 5003, 5004, 5001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_stats:current_balance\",\n",
    "        \"customer_stats:avg_passenger_count\",\n",
    "        \"customer_stats:lifetime_trip_count\",\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create stream feature view\n",
    "\n",
    "We're satisfied with the `customer_stats` feature view, as it will provide all the necessary features for a customer. But we still needs features for drivers; moreover, the streaming team has told us that there is actually a Kafka stream that can provide extremely fresh features for drivers. In order to take advantage of those fresh features, we just need to define a `StreamFeatureView` instead of a standard `FeatureView`, and pass that Kafka source to the `StreamFeatureView`. For convenience, the feature repo already contains the `KafkaSource` and `StreamFeatureView`.\n",
    "\n",
    "Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import timedelta\r\n",
      "\r\n",
      "from feast import (\r\n",
      "    FileSource,\r\n",
      "    KafkaSource,\r\n",
      ")\r\n",
      "from feast.data_format import JsonFormat\r\n",
      "\r\n",
      "# Feast also supports pulling data from data warehouses like BigQuery, Snowflake, Redshift and data lakes (e.g. via Redshift Spectrum, Trino, Spark)\r\n",
      "customer_stats_batch_source = FileSource(\r\n",
      "    name=\"customer_stats_source\",\r\n",
      "    path=\"data/customer_stats.parquet\",\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    created_timestamp_column=\"created\",\r\n",
      "    description=\"A table describing the stats of a customer based on daily logs\",\r\n",
      "    owner=\"test1@gmail.com\",\r\n",
      ")\r\n",
      "\r\n",
      "driver_stats_batch_source = FileSource(\r\n",
      "    name=\"driver_stats_source\",\r\n",
      "    path=\"data/driver_stats.parquet\",\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    created_timestamp_column=\"created\",\r\n",
      "    description=\"A table describing the stats of a driver based on hourly logs\",\r\n",
      "    owner=\"test2@gmail.com\",\r\n",
      ")\r\n",
      "\r\n",
      "driver_stats_stream_source = KafkaSource(\r\n",
      "    name=\"driver_stats_stream\",\r\n",
      "    kafka_bootstrap_servers=\"localhost:9092\",\r\n",
      "    topic=\"drivers\",\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    batch_source=driver_stats_batch_source,\r\n",
      "    message_format=JsonFormat(\r\n",
      "        schema_json=\"driver_id integer, event_timestamp timestamp, conv_rate double, acc_rate double, created timestamp\"\r\n",
      "    ),\r\n",
      "    watermark_delay_threshold=timedelta(minutes=5),\r\n",
      "    description=\"The Kafka stream containing the driver stats\",\r\n",
      "    owner=\"test3@gmail.com\",\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "!cat data_sources.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data scientist, we don't need to know too much about this Kafka stream. All we need are the bootstrap servers, the topic, and the schema to define the `KafkaSource`. We can rely on our ML platform team to ensure that the stream is up and running in production (which will be covered by a later section in this notebook). Note that we also specify a `batch_source` in the definition of this `KafkaSource`, which is the `driver_stats_batch_source` object defined above. This batch source is where our training data lives. When we want historical data to train our model, we will push from the batch source. Moreover, our ML platform team will also ensure that any streaming data that is available in production will eventually make its way into the batch source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a closer look at the stream feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import timedelta\r\n",
      "from pyspark.sql import DataFrame\r\n",
      "\r\n",
      "from feast import (\r\n",
      "    FeatureView,\r\n",
      "    Field,\r\n",
      ")\r\n",
      "from feast.stream_feature_view import stream_feature_view\r\n",
      "from feast.types import Float32, Int32\r\n",
      "\r\n",
      "from data_sources import *\r\n",
      "from entities import *\r\n",
      "\r\n",
      "customer_stats_view = FeatureView(\r\n",
      "    name=\"customer_stats\",\r\n",
      "    description=\"customer features\",\r\n",
      "    entities=[customer],\r\n",
      "    ttl=timedelta(seconds=8640000000),\r\n",
      "    schema=[\r\n",
      "        Field(name=\"current_balance\", dtype=Float32),\r\n",
      "        Field(name=\"avg_passenger_count\", dtype=Float32),\r\n",
      "        Field(name=\"lifetime_trip_count\", dtype=Int32),\r\n",
      "    ],\r\n",
      "    online=True,\r\n",
      "    source=customer_stats_batch_source,\r\n",
      "    tags={\"production\": \"True\"},\r\n",
      "    owner=\"test1@gmail.com\",\r\n",
      ")\r\n",
      "\r\n",
      "@stream_feature_view(\r\n",
      "    entities=[driver],\r\n",
      "    ttl=timedelta(seconds=8640000000),\r\n",
      "    mode=\"spark\",\r\n",
      "    schema=[\r\n",
      "        Field(name=\"conv_percentage\", dtype=Float32),\r\n",
      "        Field(name=\"acc_percentage\", dtype=Float32),\r\n",
      "    ],\r\n",
      "    timestamp_field=\"event_timestamp\",\r\n",
      "    online=True,\r\n",
      "    source=driver_stats_stream_source,\r\n",
      "    tags={},\r\n",
      ")\r\n",
      "def driver_hourly_stats_stream(df: DataFrame):\r\n",
      "    from pyspark.sql.functions import col\r\n",
      "\r\n",
      "    return (\r\n",
      "        df.withColumn(\"conv_percentage\", col(\"conv_rate\") * 100.0)\r\n",
      "        .withColumn(\"acc_percentage\", col(\"acc_rate\") * 100.0)\r\n",
      "        .drop(\"conv_rate\", \"acc_rate\")\r\n",
      "    )\r\n"
     ]
    }
   ],
   "source": [
    "!cat features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stream feature view requires mostly the same parameters as a normal feature view. One new capability with stream feature views is the ability to define an associated transformation. In this case, we can see that the transformation is a pyspark udf that will transform the rates into percentages by multiplying them by 100. This might be used in a situation where the features in the stream are in decimal format, e.g. `0.5211`, but we want to use percentages, e.g. `52.11`.\n",
    "\n",
    "Let's also inspect the historical features for the stream feature view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   driver_id                  event_timestamp  conv_percentage  acc_percentage\n",
      "0       1001        2021-04-12 10:59:42+00:00        52.114902       75.165855\n",
      "1       1003        2021-04-12 16:40:26+00:00        18.885477       34.473606\n",
      "2       1004        2021-04-12 15:01:12+00:00        29.649216       93.530525\n",
      "3       1002        2021-04-12 08:12:10+00:00         8.901370       21.263689\n",
      "4       1001 2022-06-17 11:32:55.580852+00:00        40.458847       40.757076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\",\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we need to train our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ingesting and transforming data from a Kafka topic\n",
    "\n",
    "Now let's switch to the perspective of an ML engineer. We see that a data scientist has just a stream feature view, and we are now responsible for ensuring that features from the stream are available in production.\n",
    "\n",
    "We will use Spark Structured Streaming to ingest from a Kafka topic, transform the data, and then push the data to the online store.\n",
    "\n",
    "Feast currently does not support launching and orchestrating Spark jobs to read from Kafka; any Feast user that wishes to handle streaming data must currently manage their own streaming infrastructure. As an example of what that might look like for Spark and Kafka, please see the example code [here](https://github.com/feast-dev/feast/tree/master/sdk/python/feast/infra/contrib). We will use this example code below to launch Spark jobs.\n",
    "\n",
    "We start by setting up a Spark session, as well as importing several other modules we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 11:32:58 WARN Utils: Your hostname, Felixs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.203.58.250 instead (on interface en0)\n",
      "22/06/17 11:32:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/felixwang/feast/env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/felixwang/feast/env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/felixwang/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/felixwang/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-be0e71e9-46f4-47a3-a6fc-3661ae6b71c8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 374ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-be0e71e9-46f4-47a3-a6fc-3661ae6b71c8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/9ms)\n",
      "22/06/17 11:32:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\"\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code requires three things to launch a job: a config, a stream feature view, and a function to specify where the job output will be written. Here we import the sample code and set the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast.infra.contrib.stream_processor import ProcessorConfig\n",
    "from feast.infra.contrib.spark_kafka_processor import SparkProcessorConfig\n",
    "from feast.infra.contrib.stream_processor import get_stream_processor_object\n",
    "\n",
    "spark_config = SparkProcessorConfig(mode=\"spark\", source=\"kafka\", spark_session=spark, processing_time=\"30 seconds\", query_timeout=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the method that will write data from our Kafka stream to the online store. We include several `print` statements so we can monitor what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stream_row(\n",
    "    fs: FeatureStore, \n",
    "    feature_view: str,\n",
    "    row: pd.DataFrame,\n",
    "    join_keys: List[str],\n",
    "    input_timestamp_field: str,\n",
    "    output_timestamp_column: str = \"\",\n",
    "):\n",
    "    row = (\n",
    "        row.sort_values(by=join_keys + [input_timestamp_field], ascending=True)\n",
    "        .groupby(join_keys)\n",
    "        .nth(0)\n",
    "    )\n",
    "    if output_timestamp_column and output_timestamp_column != input_timestamp_field:\n",
    "        row = row.rename(columns={input_timestamp_field, output_timestamp_column})\n",
    "    row[\"created\"] = pd.to_datetime(\"now\", utc=True)\n",
    "    \n",
    "    print(f\"df columns: {row.columns}\")\n",
    "    print(f\"df size: {row.size}\")\n",
    "    print(f\"df preview:\\n{row.head()}\")\n",
    "    \n",
    "    if row.size > 0:\n",
    "        print(\"writing row\")\n",
    "        fs.write_to_online_store(\n",
    "            feature_view, row,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple wrapper method that takes our feature store, the name of a stream feature view, and the config for a stream processor, and launches an ingestion job. It also returns a handler for the ingestion job, which can be used to monitor and stop the job as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_stream_feature_view(fs: FeatureStore, sfv_name: str, processor_config: ProcessorConfig):\n",
    "    sfv = fs.get_stream_feature_view(sfv_name)\n",
    "    join_keys = [fs.get_entity(entity).join_key for entity in sfv.entities]\n",
    "    \n",
    "    processor = get_stream_processor_object(\n",
    "        config=processor_config,\n",
    "        sfv=sfv,\n",
    "        write_function=lambda row, input_timestamp, output_timestamp: write_stream_row(\n",
    "            fs=fs,\n",
    "            feature_view=sfv_name,\n",
    "            row=row,\n",
    "            join_keys=join_keys,\n",
    "            input_timestamp_field=input_timestamp,\n",
    "            output_timestamp_column=output_timestamp,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    query = processor.ingest_stream_feature_view()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready! \n",
    "\n",
    "But before launching an ingestion job for the `driver_hourly_stats_stream` stream feature view, let's materialize the historical features, since we want to confirm that our ingestion job is actually updating the feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/specifiers.py:273: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  warnings.warn(\n",
      "/Users/felixwang/feast/env/lib/python3.8/site-packages/pkg_resources/_vendor/packaging/version.py:111: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  warnings.warn(\n",
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "Materializing \u001b[1m\u001b[32m2\u001b[0m feature views to \u001b[1m\u001b[32m2022-06-16 17:00:00-07:00\u001b[0m into the \u001b[1m\u001b[32mredis\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mcustomer_stats\u001b[0m from \u001b[1m\u001b[32m1748-09-01 18:33:08-07:52:58\u001b[0m to \u001b[1m\u001b[32m2022-06-16 17:00:00-07:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 398.85it/s]\n",
      "\u001b[1m\u001b[32mdriver_hourly_stats_stream\u001b[0m from \u001b[1m\u001b[32m1748-09-01 18:33:08-07:52:58\u001b[0m to \u001b[1m\u001b[32m2022-06-16 17:00:00-07:00\u001b[0m:\n",
      "100%|████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 928.15it/s]\n",
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the features have been materialized correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_percentage  :  [40.757076263427734]\n",
      "conv_percentage  :  [40.45884704589844]\n",
      "driver_id  :  [1001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we launch the ingestion job. Let's let it run for a little bit to ensure that the job is indeed pushing data to the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 11:33:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: Index(['event_timestamp', 'created', 'conv_percentage', 'acc_percentage'], dtype='object')\n",
      "df size: 0\n",
      "df preview:\n",
      "Empty DataFrame\n",
      "Columns: [event_timestamp, created, conv_percentage, acc_percentage]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: Index(['event_timestamp', 'created', 'conv_percentage', 'acc_percentage'], dtype='object')\n",
      "df size: 20\n",
      "df preview:\n",
      "              event_timestamp                          created  \\\n",
      "driver_id                                                        \n",
      "1001      2024-02-26 08:00:00 2022-06-17 18:33:31.261210+00:00   \n",
      "1002      2024-02-26 08:00:00 2022-06-17 18:33:31.261210+00:00   \n",
      "1003      2024-02-26 08:00:00 2022-06-17 18:33:31.261210+00:00   \n",
      "1004      2024-02-26 08:00:00 2022-06-17 18:33:31.261210+00:00   \n",
      "1005      2024-02-26 08:00:00 2022-06-17 18:33:31.261210+00:00   \n",
      "\n",
      "           conv_percentage  acc_percentage  \n",
      "driver_id                                   \n",
      "1001             81.257027       60.436976  \n",
      "1002             85.917443       89.079225  \n",
      "1003             23.591906       90.642333  \n",
      "1004             33.968881        6.300516  \n",
      "1005              4.525895       70.851427  \n",
      "writing row\n"
     ]
    }
   ],
   "source": [
    "query = ingest_stream_feature_view(store, \"driver_hourly_stats_stream\", spark_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we retrieve features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_percentage  :  [60.43697738647461]\n",
      "conv_percentage  :  [81.25702667236328]\n",
      "driver_id  :  [1001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixwang/feast/sdk/python/feast/stream_feature_view.py:98: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats_stream:conv_percentage\",\n",
    "        \"driver_hourly_stats_stream:acc_percentage\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can clearly see that the new features are more fresh! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Finally, let's clean up the checkpoint directories from Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/checkpoint'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Closing thoughts and future work\n",
    "\n",
    "Although this workflow will allow data scientists to take advantage of streaming features, there are still many ways it can be improved. Here are a few things the Feast team is planning to work on in the near future:\n",
    "* unified push API\n",
    "* aggregations DSL\n",
    "* the ability to run the stream transformation in batch mode to avoid the log and wait approach"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d634b9af180bcb32a446a43848522733ff8f5bbf0cc46dba1a83bede04bf237"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
