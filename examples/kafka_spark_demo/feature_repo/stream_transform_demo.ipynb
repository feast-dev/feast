{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606c5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/kevinzhang/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /Users/kevinzhang/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kevinzhang/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-684357ec-52b7-4e6c-9d83-35406873acff;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kevinzhang/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 185ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-684357ec-52b7-4e6c-9d83-35406873acff\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/4ms)\n",
      "22/06/08 12:05:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/08 12:05:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "from pyspark.sql import SparkSession\n",
    "from feast.stream_processor import SparkProcessorConfig\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\"\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354a470",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbddf3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kevinzhang/tecton-ai/offline_store/feast/sdk/python/feast/feature_view.py:178: DeprecationWarning: The `entities` parameter should be a list of `Entity` objects. Feast 0.23 and onwards will not support passing in a list of strings to define entities.\n",
      "  warnings.warn(\n",
      "/Users/kevinzhang/tecton-ai/offline_store/feast/sdk/python/feast/stream_feature_view.py:61: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "\u001b[1m\u001b[94mNo changes to registry\n",
      "\u001b[1m\u001b[94mNo changes to infrastructure\n"
     ]
    }
   ],
   "source": [
    "!feast apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67021de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing \u001b[1m\u001b[32m1\u001b[0m feature views to \u001b[1m\u001b[32m2022-06-07 17:00:00-07:00\u001b[0m into the \u001b[1m\u001b[32msqlite\u001b[0m online store.\n",
      "\n",
      "\u001b[1m\u001b[32mnormal_stats\u001b[0m from \u001b[1m\u001b[32m2022-06-06 17:00:00-07:00\u001b[0m to \u001b[1m\u001b[32m2022-06-07 17:00:00-07:00\u001b[0m:\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c11473",
   "metadata": {},
   "source": [
    "#### The normal_stats feature view contains the data without streaming directly from the file parquet file. We fetch some feature data here from the normal_stats feature view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0485db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "acc_rate__ts  :  [1647266400]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "conv_rate__ts  :  [1647266400]\n",
      "driver_id  :  [1001]\n",
      "driver_id__ts  :  [0]\n",
      "miles_driven  :  [46.53329086303711]\n",
      "miles_driven__ts  :  [1647266400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinzhang/tecton-ai/offline_store/feast/sdk/python/feast/stream_feature_view.py:61: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fs = FeatureStore(\".\")\n",
    "\n",
    "features = fs.get_online_features(\n",
    "    features=[\n",
    "        \"normal_stats:conv_rate\",\n",
    "        \"normal_stats:acc_rate\",\n",
    "        \"normal_stats:miles_driven\"\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict(include_event_timestamps=True)\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fa79919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StreamFeatureView(aggregations = [], mode = spark, timestamp_field = , udf = None, name = driver_hourly_stats, entities = ['driver'], stream_source = {\n",
       "  \"type\": \"STREAM_KAFKA\",\n",
       "  \"timestampField\": \"event_timestamp\",\n",
       "  \"kafkaOptions\": {\n",
       "    \"bootstrapServers\": \"localhost:9092\",\n",
       "    \"topic\": \"drivers\",\n",
       "    \"messageFormat\": {\n",
       "      \"jsonFormat\": {\n",
       "        \"schemaJson\": \"driver_id integer, miles_driven double, event_timestamp timestamp, conv_rate double, acc_rate double\"\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  \"name\": \"driver_stats_stream\",\n",
       "  \"batchSource\": {\n",
       "    \"type\": \"BATCH_FILE\",\n",
       "    \"timestampField\": \"event_timestamp\",\n",
       "    \"createdTimestampColumn\": \"created\",\n",
       "    \"fileOptions\": {\n",
       "      \"uri\": \"data/driver_stats.parquet\"\n",
       "    },\n",
       "    \"name\": \"data/driver_stats.parquet\"\n",
       "  }\n",
       "}, batch_source = {\n",
       "  \"type\": \"BATCH_FILE\",\n",
       "  \"timestampField\": \"event_timestamp\",\n",
       "  \"createdTimestampColumn\": \"created\",\n",
       "  \"fileOptions\": {\n",
       "    \"uri\": \"data/driver_stats.parquet\"\n",
       "  },\n",
       "  \"name\": \"data/driver_stats.parquet\"\n",
       "}, source = {\n",
       "  \"type\": \"STREAM_KAFKA\",\n",
       "  \"timestampField\": \"event_timestamp\",\n",
       "  \"kafkaOptions\": {\n",
       "    \"bootstrapServers\": \"localhost:9092\",\n",
       "    \"topic\": \"drivers\",\n",
       "    \"messageFormat\": {\n",
       "      \"jsonFormat\": {\n",
       "        \"schemaJson\": \"driver_id integer, miles_driven double, event_timestamp timestamp, conv_rate double, acc_rate double\"\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  \"name\": \"driver_stats_stream\",\n",
       "  \"batchSource\": {\n",
       "    \"type\": \"BATCH_FILE\",\n",
       "    \"timestampField\": \"event_timestamp\",\n",
       "    \"createdTimestampColumn\": \"created\",\n",
       "    \"fileOptions\": {\n",
       "      \"uri\": \"data/driver_stats.parquet\"\n",
       "    },\n",
       "    \"name\": \"data/driver_stats.parquet\"\n",
       "  }\n",
       "}, ttl = 1 day, 0:00:00, schema = [conv_rate-Float32, acc_rate-Float32, miles_driven-Float32], entity_columns = [], features = [conv_rate-Float32, acc_rate-Float32, miles_driven-Float32], description = , tags = {}, owner = , projection = FeatureViewProjection(name='driver_hourly_stats', name_alias=None, features=[conv_rate-Float32, acc_rate-Float32, miles_driven-Float32], join_key_map={}), created_timestamp = 2022-06-07 20:43:21.306316, last_updated_timestamp = 2022-06-07 20:43:21.306316, online = True, materialization_intervals = [])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfv = fs.list_stream_feature_views()[0]\n",
    "sfv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258130f",
   "metadata": {},
   "source": [
    "#### Set up the configuration of your spark node/cluster and pass in a SparkSession object. \n",
    "- Pass in the processor config and ingest stream feature view data from the kafka source data. \n",
    "- Since no udfs are used, the dataframe that is created will be modified and written to the online store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59cc9db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/07 13:43:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "/Users/kevinzhang/tecton-ai/offline_store/feast/sdk/python/feast/stream_feature_view.py:61: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kevinzhang/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/kevinzhang/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/kevinzhang/.pyenv/versions/3.8.12/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# should only return miles_driven since there is no fresher data for conv_rate and acc_rate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m SparkProcessorConfig(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m, spark_session\u001b[38;5;241m=\u001b[39mspark)\n\u001b[0;32m----> 3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_stream_feature_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43msfv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msfv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tecton-ai/offline_store/feast/sdk/python/feast/feature_store.py:1419\u001b[0m, in \u001b[0;36mFeatureStore.ingest_stream_feature_view\u001b[0;34m(self, sfv_name, processor_config)\u001b[0m\n\u001b[1;32m   1401\u001b[0m join_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entity(entity, allow_registry_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mjoin_key\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m stream_feature_view\u001b[38;5;241m.\u001b[39mentities\n\u001b[1;32m   1404\u001b[0m ]\n\u001b[1;32m   1406\u001b[0m processor \u001b[38;5;241m=\u001b[39m get_stream_processor_object(\n\u001b[1;32m   1407\u001b[0m     config\u001b[38;5;241m=\u001b[39mprocessor_config,\n\u001b[1;32m   1408\u001b[0m     sfv\u001b[38;5;241m=\u001b[39mstream_feature_view,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     ),\n\u001b[1;32m   1417\u001b[0m )\n\u001b[0;32m-> 1419\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mingest_stream_feature_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;66;03m# Handle query(set up monitoring thread, etc)\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/tecton-ai/offline_store/feast/sdk/python/feast/stream_processor.py:198\u001b[0m, in \u001b[0;36mSparkStreamKafkaProcessor.ingest_stream_feature_view\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m ingested_stream_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ingest_stream_data()\n\u001b[1;32m    197\u001b[0m transformed_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_transformation_plan(ingested_stream_df)\n\u001b[0;32m--> 198\u001b[0m online_store_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_to_online_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m online_store_query\n",
      "File \u001b[0;32m~/tecton-ai/offline_store/feast/sdk/python/feast/stream_processor.py:188\u001b[0m, in \u001b[0;36mSparkStreamKafkaProcessor._write_to_online_store\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_function(\n\u001b[1;32m    178\u001b[0m         pd_row, input_timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_timestamp\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    181\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    182\u001b[0m     df\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/checkpoint/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    187\u001b[0m )\n\u001b[0;32m--> 188\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/pyspark/sql/streaming.py:99\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(timeout, (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout must be a positive integer or float. Got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout)\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/feast_env/lib/python3.8/site-packages/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# should only return miles_driven since there is no fresher data for conv_rate and acc_rate\n",
    "config = SparkProcessorConfig(mode=\"spark\", source=\"kafka\", spark_session=spark)\n",
    "query = fs.ingest_stream_feature_view(sfv_name=sfv.name, processor_config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b064c9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkStreamKafkaProcessor' object has no attribute 'udf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# should only return miles_driven since there is no fresher data for conv_rate and acc_rate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m SparkProcessorConfig(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m, spark_session\u001b[38;5;241m=\u001b[39mspark)\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_stream_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msfv_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msfv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregates\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mstart() \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m~/tecton-ai/offline_store/feast/sdk/python/feast/feature_store.py:1417\u001b[0m, in \u001b[0;36mFeatureStore.transform_stream_data\u001b[0;34m(self, sfv_name, processor_config)\u001b[0m\n\u001b[1;32m   1399\u001b[0m join_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_entity(entity, allow_registry_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mjoin_key\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m stream_feature_view\u001b[38;5;241m.\u001b[39mentities\n\u001b[1;32m   1402\u001b[0m ]\n\u001b[1;32m   1404\u001b[0m processor \u001b[38;5;241m=\u001b[39m get_stream_processor_object(\n\u001b[1;32m   1405\u001b[0m     config\u001b[38;5;241m=\u001b[39mprocessor_config,\n\u001b[1;32m   1406\u001b[0m     sfv\u001b[38;5;241m=\u001b[39mstream_feature_view,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     ),\n\u001b[1;32m   1415\u001b[0m )\n\u001b[0;32m-> 1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_stream_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tecton-ai/offline_store/feast/sdk/python/feast/stream_processor.py:213\u001b[0m, in \u001b[0;36mSparkStreamKafkaProcessor.transform_stream_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_stream_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StreamTable:\n\u001b[1;32m    212\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ingest_stream_data()\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_transformation_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tecton-ai/offline_store/feast/sdk/python/feast/stream_processor.py:166\u001b[0m, in \u001b[0;36mSparkStreamKafkaProcessor._construct_transformation_plan\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_construct_transformation_plan\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: StreamTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StreamTable:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Applies transformations on top of StreamTable object. Since stream engines use lazy\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    evaluation, the StreamTable will not be materialized until it is actually evaluated.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    For example: df.collect() in spark or tbl.execute() in Flink.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mudf\u001b[49m:\n\u001b[1;32m    167\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mudf(df)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# TODO(kevjumba) fix\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkStreamKafkaProcessor' object has no attribute 'udf'"
     ]
    }
   ],
   "source": [
    "# should only return miles_driven since there is no fresher data for conv_rate and acc_rate\n",
    "config = SparkProcessorConfig(mode=\"spark\", source=\"kafka\", spark_session=spark)\n",
    "df = fs.transform_stream_data(sfv_name=sfv.name, processor_config=config)\n",
    "df.writeStream \\\n",
    "    .queryName(\"aggregates\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b77e2",
   "metadata": {},
   "source": [
    "### Retrieve features from the driver_hourly_stats stream feature view.\n",
    "- You should see different miles_driven features than the ones you saw in normal stats since the data being pulled is edited to have a fresher(newer) creation timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcffab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [None]\n",
      "acc_rate__ts  :  [1678006800]\n",
      "conv_rate  :  [None]\n",
      "conv_rate__ts  :  [1678006800]\n",
      "driver_id  :  [1001]\n",
      "driver_id__ts  :  [0]\n",
      "miles_driven  :  [45.377933502197266]\n",
      "miles_driven__ts  :  [1678006800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinzhang/tecton-ai/offline_store/feast/sdk/python/feast/stream_feature_view.py:61: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "/Users/kevinzhang/tecton-ai/offline_store/feast/sdk/python/feast/stream_feature_view.py:61: RuntimeWarning: Stream Feature Views are experimental features in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# streaming input has rows of miles_driven features that are more fresh than the ones in the batch source\n",
    "features = fs.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:miles_driven\"\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict(include_event_timestamps=True)\n",
    "\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284d9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
