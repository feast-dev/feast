package logging

import (
	"errors"
	"fmt"
	"log"
	"time"

	"github.com/apache/arrow/go/v8/arrow"
	"github.com/apache/arrow/go/v8/arrow/array"
	"github.com/apache/arrow/go/v8/arrow/memory"
	"github.com/feast-dev/feast/go/internal/feast"
	"github.com/feast-dev/feast/go/protos/feast/serving"
	"github.com/feast-dev/feast/go/protos/feast/types"
	gotypes "github.com/feast-dev/feast/go/types"
	"google.golang.org/protobuf/types/known/timestamppb"
)

type Log struct {
	// Example: val{int64_val: 5017}, val{int64_val: 1003}
	EntityValue []*types.Value

	FeatureValues   []*types.Value
	FeatureStatuses []serving.FieldStatus
	EventTimestamps []*timestamppb.Timestamp
	RequestContext  map[string]*types.RepeatedValue
}

type MemoryBuffer struct {
	featureService *feast.FeatureService
	logs           []*Log
}

type LoggingService struct {
	memoryBuffer      *MemoryBuffer
	logChannel        chan *Log
	fs                *feast.FeatureStore
	offlineLogStorage OfflineLogStorage
}

func NewLoggingService(fs *feast.FeatureStore, logChannelCapacity int, featureServiceName string, enableLogging bool) (*LoggingService, error) {
	// start handler processes?
	var featureService *feast.FeatureService = nil
	var err error
	if enableLogging {
		featureService, err = fs.GetFeatureService(featureServiceName, fs.GetRepoConfig().Project)
		if err != nil {
			return nil, err
		}
	}

	loggingService := &LoggingService{
		logChannel: make(chan *Log, logChannelCapacity),
		memoryBuffer: &MemoryBuffer{
			logs:           make([]*Log, 0),
			featureService: featureService,
		},
		fs: fs,
	}

	if !enableLogging || fs == nil {
		loggingService.offlineLogStorage = nil
	} else {
		offlineLogStorage, err := NewOfflineStore(fs.GetRepoConfig())
		loggingService.offlineLogStorage = offlineLogStorage

		if err != nil {
			return nil, err
		}
		// Start goroutine to process logs
		go loggingService.processLogs()
	}
	return loggingService, nil
}

func (s *LoggingService) EmitLog(l *Log) error {
	select {
	case s.logChannel <- l:
		return nil
	case <-time.After(20 * time.Millisecond):
		return fmt.Errorf("could not add to log channel with capacity %d. Operation timed out. Current log channel length is %d", cap(s.logChannel), len(s.logChannel))
	}
}

func (s *LoggingService) processLogs() {
	// start a periodic flush
	// TODO(kevjumba): set param so users can configure flushing duration
	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()

	for {
		s.ProcessMemoryBuffer(ticker)
	}
}

func (s *LoggingService) ProcessMemoryBuffer(t *time.Ticker) {
	select {
	case t := <-t.C:
		s.flushLogsToOfflineStorage(t)
	case new_log := <-s.logChannel:
		log.Printf("Pushing %s to memory.\n", new_log.FeatureValues)
		s.memoryBuffer.logs = append(s.memoryBuffer.logs, new_log)
	}
}

func (s *LoggingService) flushLogsToOfflineStorage(t time.Time) error {
	log.Printf("Flushing buffer to offline storage with channel length: %d\n at time: "+t.String(), len(s.memoryBuffer.logs))
	offlineStoreType, ok := getOfflineStoreType(s.fs.GetRepoConfig().OfflineStore)
	if !ok {
		return fmt.Errorf("could not get offline storage type for config: %s", s.fs.GetRepoConfig().OfflineStore)
	}
	if offlineStoreType == "file" {
		entities, featureViews, odfvs, err := s.GetFcos()
		if err != nil {
			return err
		}
		schema, err := GetTypesFromFeatureService(s.memoryBuffer.featureService, entities, featureViews, odfvs)
		if err != nil {
			return err
		}
		table, err := ConvertMemoryBufferToArrowTable(s.memoryBuffer, schema)
		if err != nil {
			return err
		}
		fileStore, err := NewFileOfflineStore(s.fs.GetRepoConfig().Project, s.fs.GetRepoConfig().OfflineStore)
		if err != nil {
			return err
		}
		fileStore.FlushToStorage(table)
		s.memoryBuffer.logs = s.memoryBuffer.logs[:0]
	} else {
		// Currently don't support any other offline flushing.
		return errors.New("currently only file type is supported for offline log storage")
	}
	return nil
}

// Takes memory buffer of logs in array row and converts them to columnar with generated fcoschema generated by GetFcoSchema
// and writes them to arrow table.
// Returns arrow table that contains all of the logs in columnar format.
func ConvertMemoryBufferToArrowTable(memoryBuffer *MemoryBuffer, fcoSchema *Schema) (array.Table, error) {
	arrowMemory := memory.NewGoAllocator()

	columnNameToProtoValueArray := make(map[string][]*types.Value)
	columnNameToStatus := make(map[string][]int32)
	columnNameToTimestamp := make(map[string][]int64)
	entityNameToEntityValues := make(map[string][]*types.Value)

	for _, l := range memoryBuffer.logs {
		// EntityTypes maps an entity name to the specific type and also which index in the entityValues array it is
		// e.g if an Entity Key is {driver_id, customer_id}, then the driver_id entitytype would be dtype=int64, index=0.
		// It's in the order of the entities as given by the schema.
		for idx, entityName := range fcoSchema.Entities {
			if _, ok := entityNameToEntityValues[entityName]; !ok {
				entityNameToEntityValues[entityName] = make([]*types.Value, 0)
			}
			entityNameToEntityValues[entityName] = append(entityNameToEntityValues[entityName], l.EntityValue[idx])
		}

		// Contains both fv and odfv feature value types => add them in order of how the appear in the featureService
		for idx, featureName := range fcoSchema.Features {
			// for featureName, idAndType := range fcoSchema.FeaturesTypes {
			// populate the proto value arrays with values from memory buffer in separate columns one for each feature name
			if _, ok := columnNameToProtoValueArray[featureName]; !ok {
				columnNameToProtoValueArray[featureName] = make([]*types.Value, 0)
				columnNameToStatus[featureName] = make([]int32, 0)
				columnNameToTimestamp[featureName] = make([]int64, 0)
			}
			columnNameToProtoValueArray[featureName] = append(columnNameToProtoValueArray[featureName], l.FeatureValues[idx])
			columnNameToStatus[featureName] = append(columnNameToStatus[featureName], int32(l.FeatureStatuses[idx]))
			columnNameToTimestamp[featureName] = append(columnNameToTimestamp[featureName], l.EventTimestamps[idx].AsTime().UnixNano()/int64(time.Millisecond))
		}
	}

	fields := make([]arrow.Field, 0)
	columns := make([]array.Interface, 0)
	for _, entityName := range fcoSchema.Entities {
		protoArr := entityNameToEntityValues[entityName]
		if len(protoArr) == 0 {
			break
		}
		valArrowArray, err := gotypes.ProtoValuesToArrowArray(protoArr, arrowMemory, len(columnNameToProtoValueArray))
		if err != nil {
			return nil, err
		}
		arrowType, err := gotypes.ValueTypeEnumToArrowType(fcoSchema.EntityTypes[entityName])
		if err != nil {
			return nil, err
		}
		fields = append(fields, arrow.Field{
			Name: entityName,
			Type: arrowType,
		})
		columns = append(columns, valArrowArray)
	}

	for _, featureName := range fcoSchema.Features {

		protoArr := columnNameToProtoValueArray[featureName]
		if len(protoArr) == 0 {
			break
		}
		arrowArray, err := gotypes.ProtoValuesToArrowArray(protoArr, arrowMemory, len(columnNameToProtoValueArray))
		if err != nil {
			return nil, err
		}

		arrowType, err := gotypes.ValueTypeEnumToArrowType(fcoSchema.FeaturesTypes[featureName])

		if err != nil {
			return nil, err
		}
		fields = append(fields, arrow.Field{
			Name: featureName,
			Type: arrowType,
		})
		columns = append(columns, arrowArray)
	}
	schema := arrow.NewSchema(
		fields,
		nil,
	)

	result := array.Record(array.NewRecord(schema, columns, int64(len(memoryBuffer.logs))))
	// create an arrow table -> write this to parquet.

	tbl := array.NewTableFromRecords(schema, []array.Record{result})
	// arrow table -> write this to parquet
	return array.Table(tbl), nil
}

type Schema struct {
	Entities      []string
	Features      []string
	EntityTypes   map[string]types.ValueType_Enum
	FeaturesTypes map[string]types.ValueType_Enum
}

func GetTypesFromFeatureService(featureService *feast.FeatureService, entities []*feast.Entity, featureViews []*feast.FeatureView, onDemandFeatureViews []*feast.OnDemandFeatureView) (*Schema, error) {
	fvs := make(map[string]*feast.FeatureView)
	odFvs := make(map[string]*feast.OnDemandFeatureView)

	entityNames := make([]string, 0)
	for _, entity := range entities {
		entityNames = append(entityNames, entity.Joinkey)
	}
	//featureViews, err := fs.listFeatureViews(hideDummyEntity)

	for _, featureView := range featureViews {
		fvs[featureView.Base.Name] = featureView
	}

	for _, onDemandFeatureView := range onDemandFeatureViews {
		odFvs[onDemandFeatureView.Base.Name] = onDemandFeatureView
	}

	entityJoinKeyToType := make(map[string]types.ValueType_Enum)
	for _, entity := range entities {
		entityJoinKeyToType[entity.Joinkey] = entity.Valuetype
	}

	allFeatureTypes := make(map[string]types.ValueType_Enum)
	//allRequestDataTypes := make(map[string]*types.ValueType_Enum)
	features := make([]string, 0)
	for _, featureProjection := range featureService.Projections {
		// Create copies of FeatureView that may contains the same *FeatureView but
		// each differentiated by a *FeatureViewProjection
		featureViewName := featureProjection.Name
		if fv, ok := fvs[featureViewName]; ok {
			for _, f := range fv.Base.Features {
				// add feature to map
				features = append(features, f.Name)
				allFeatureTypes[f.Name] = f.Dtype
			}
		} else if odfv, ok := odFvs[featureViewName]; ok {
			for name, valueType := range odfv.GetRequestDataSchema() {
				features = append(features, name)
				allFeatureTypes[name] = valueType
			}
		} else {
			return nil, fmt.Errorf("no such feature view found in feature service %s", featureViewName)
		}
	}
	schema := &Schema{
		Entities:      entityNames,
		Features:      features,
		EntityTypes:   entityJoinKeyToType,
		FeaturesTypes: allFeatureTypes,
	}
	return schema, nil
}

func (s *LoggingService) GetFcos() ([]*feast.Entity, []*feast.FeatureView, []*feast.OnDemandFeatureView, error) {
	odfvs, err := s.fs.ListOnDemandFeatureViews()
	if err != nil {
		return nil, nil, nil, err
	}
	fvs, err := s.fs.ListFeatureViews()
	if err != nil {
		return nil, nil, nil, err
	}
	entities, err := s.fs.ListEntities(true)
	if err != nil {
		return nil, nil, nil, err
	}
	return entities, fvs, odfvs, nil
}
