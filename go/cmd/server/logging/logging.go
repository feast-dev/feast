package logging

import (
	"errors"
	"fmt"
	"log"
	"time"

	"github.com/apache/arrow/go/v8/arrow"
	"github.com/apache/arrow/go/v8/arrow/array"
	"github.com/apache/arrow/go/v8/arrow/memory"
	"github.com/feast-dev/feast/go/internal/feast"
	"github.com/feast-dev/feast/go/internal/feast/model"
	"github.com/feast-dev/feast/go/protos/feast/serving"
	"github.com/feast-dev/feast/go/protos/feast/types"
	gotypes "github.com/feast-dev/feast/go/types"
	"google.golang.org/protobuf/types/known/timestamppb"
)

const DEFAULT_LOG_FLUSH_INTERVAL = 100 * time.Millisecond
const DEFAULT_LOG_INSERT_TIMEOUT = 20 * time.Millisecond

type Log struct {
	// Example: val{int64_val: 5017}, val{int64_val: 1003}
	EntityValue []*types.Value

	FeatureValues   []*types.Value
	FeatureStatuses []serving.FieldStatus
	EventTimestamps []*timestamppb.Timestamp
	RequestContext  map[string]*types.Value
	RequestId       string
}

type MemoryBuffer struct {
	featureService *model.FeatureService
	logs           []*Log
}

type LoggingService struct {
	memoryBuffer      *MemoryBuffer
	logChannel        chan *Log
	fs                *feast.FeatureStore
	offlineLogStorage OfflineLogStorage
	logInsertTTl      time.Duration
	logFlushInterval  time.Duration
}

func NewLoggingService(fs *feast.FeatureStore, logChannelCapacity int, featureServiceName string, enableLogProcessing bool) (*LoggingService, error) {
	var featureService *model.FeatureService = nil
	var err error
	if fs != nil {
		featureService, err = fs.GetFeatureService(featureServiceName)
		if err != nil {
			return nil, err
		}

	}

	loggingService := &LoggingService{
		logChannel: make(chan *Log, logChannelCapacity),
		memoryBuffer: &MemoryBuffer{
			logs:           make([]*Log, 0),
			featureService: featureService,
		},
		fs:               fs,
		logInsertTTl:     DEFAULT_LOG_INSERT_TIMEOUT,
		logFlushInterval: DEFAULT_LOG_FLUSH_INTERVAL,
	}

	if fs != nil {
		offlineLogStorage, err := NewOfflineStore(fs.GetRepoConfig())
		loggingService.offlineLogStorage = offlineLogStorage
		if err != nil {
			return nil, err
		}
	}

	// Start goroutine to process logs
	if enableLogProcessing {
		go loggingService.processLogs()

	}
	return loggingService, nil
}

func (s *LoggingService) EmitLog(l *Log) error {
	select {
	case s.logChannel <- l:
		return nil
	case <-time.After(s.logInsertTTl):
		return fmt.Errorf("could not add to log channel with capacity %d. Operation timed out. Current log channel length is %d", cap(s.logChannel), len(s.logChannel))
	}
}

func (s *LoggingService) processLogs() {
	// start a periodic flush
	// TODO(kevjumba): set param so users can configure flushing duration
	ticker := time.NewTicker(s.logFlushInterval)
	defer ticker.Stop()

	for {
		s.PerformPeriodicAppendToMemoryBufferAndLogFlush(ticker)
	}
}

// Select that either ingests new logs that are added to the logging channel, one at a time to add
// to the in-memory buffer or flushes all of them synchronously to the OfflineStorage on a time interval.
func (s *LoggingService) PerformPeriodicAppendToMemoryBufferAndLogFlush(t *time.Ticker) {
	select {
	case t := <-t.C:
		s.flushLogsToOfflineStorage(t)
	case new_log := <-s.logChannel:
		log.Printf("Adding %s to memory.\n", new_log.FeatureValues)
		s.memoryBuffer.logs = append(s.memoryBuffer.logs, new_log)
	}
}

// Acquires the logging schema from the feature service, converts the memory buffer array of rows of logs and flushes
// them to the offline storage.
func (s *LoggingService) flushLogsToOfflineStorage(t time.Time) error {
	offlineStoreType, ok := getOfflineStoreType(s.fs.GetRepoConfig().OfflineStore)
	if !ok {
		return fmt.Errorf("could not get offline storage type for config: %s", s.fs.GetRepoConfig().OfflineStore)
	}
	if offlineStoreType == "file" {
		entityMap, featureViews, odfvs, err := s.GetFcos()
		if err != nil {
			return err
		}
		schema, err := GetSchemaFromFeatureService(s.memoryBuffer.featureService, entityMap, featureViews, odfvs)
		if err != nil {
			return err
		}
		table, err := ConvertMemoryBufferToArrowTable(s.memoryBuffer, schema)
		if err != nil {
			return err
		}
		s.offlineLogStorage.FlushToStorage(table)
		if err != nil {
			return err
		}
		s.memoryBuffer.logs = s.memoryBuffer.logs[:0]
	} else {
		// Currently don't support any other offline flushing.
		return errors.New("currently only file type is supported for offline log storage")
	}
	return nil
}

// Takes memory buffer of logs in array row and converts them to columnar with generated fcoschema generated by GetFcoSchema
// and writes them to arrow table.
// Returns arrow table that contains all of the logs in columnar format.
func ConvertMemoryBufferToArrowTable(memoryBuffer *MemoryBuffer, fcoSchema *Schema) (array.Table, error) {
	arrowMemory := memory.NewGoAllocator()

	columnNameToProtoValueArray := make(map[string][]*types.Value)
	columnNameToStatus := make(map[string][]int32)
	columnNameToTimestamp := make(map[string][]int64)
	entityNameToEntityValues := make(map[string][]*types.Value)

	strBuilder := array.NewStringBuilder(arrowMemory)

	for _, l := range memoryBuffer.logs {
		// EntityTypes maps an entity name to the specific type and also which index in the entityValues array it is
		// e.g if an Entity Key is {driver_id, customer_id}, then the driver_id entitytype would be dtype=int64, index=0.
		// It's in the order of the entities as given by the schema.
		for idx, joinKey := range fcoSchema.Entities {
			if _, ok := entityNameToEntityValues[joinKey]; !ok {
				entityNameToEntityValues[joinKey] = make([]*types.Value, 0)
			}
			entityNameToEntityValues[joinKey] = append(entityNameToEntityValues[joinKey], l.EntityValue[idx])
		}

		// Contains both fv and odfv feature value types => they are processed in order of how the appear in the featureService
		for idx, featureName := range fcoSchema.Features {
			// populate the proto value arrays with values from memory buffer in separate columns one for each feature name
			if _, ok := columnNameToProtoValueArray[featureName]; !ok {
				columnNameToProtoValueArray[featureName] = make([]*types.Value, 0)
				columnNameToStatus[featureName] = make([]int32, 0)
				columnNameToTimestamp[featureName] = make([]int64, 0)
			}
			columnNameToProtoValueArray[featureName] = append(columnNameToProtoValueArray[featureName], l.FeatureValues[idx])
			columnNameToStatus[featureName] = append(columnNameToStatus[featureName], int32(l.FeatureStatuses[idx]))
			columnNameToTimestamp[featureName] = append(columnNameToTimestamp[featureName], l.EventTimestamps[idx].AsTime().UnixNano()/int64(time.Millisecond))
		}
		strBuilder.Append(l.RequestId)
	}

	fields := make([]arrow.Field, 0)
	columns := make([]array.Interface, 0)
	for _, entityName := range fcoSchema.Entities {
		protoArr := entityNameToEntityValues[entityName]
		if len(protoArr) == 0 {
			break
		}
		valArrowArray, err := gotypes.ProtoValuesToArrowArray(protoArr, arrowMemory, len(columnNameToProtoValueArray))
		if err != nil {
			return nil, err
		}
		arrowType, err := gotypes.ValueTypeEnumToArrowType(fcoSchema.EntityTypes[entityName])
		if err != nil {
			return nil, err
		}
		fields = append(fields, arrow.Field{
			Name: entityName,
			Type: arrowType,
		})
		columns = append(columns, valArrowArray)
	}

	for _, featureName := range fcoSchema.Features {

		protoArr := columnNameToProtoValueArray[featureName]
		if len(protoArr) == 0 {
			break
		}
		arrowArray, err := gotypes.ProtoValuesToArrowArray(protoArr, arrowMemory, len(columnNameToProtoValueArray))
		if err != nil {
			return nil, err
		}

		arrowType, err := gotypes.ValueTypeEnumToArrowType(fcoSchema.FeaturesTypes[featureName])

		if err != nil {
			return nil, err
		}
		fields = append(fields, arrow.Field{
			Name: featureName,
			Type: arrowType,
		})
		columns = append(columns, arrowArray)
	}
	fields = append(fields, arrow.Field{
		Name: "RequestId",
		Type: &arrow.StringType{},
	})

	columns = append(columns, strBuilder.NewArray())
	schema := arrow.NewSchema(
		fields,
		nil,
	)

	result := array.Record(array.NewRecord(schema, columns, int64(len(memoryBuffer.logs))))

	tbl := array.NewTableFromRecords(schema, []array.Record{result})
	return array.Table(tbl), nil
}

type Schema struct {
	Entities      []string
	Features      []string
	EntityTypes   map[string]types.ValueType_Enum
	FeaturesTypes map[string]types.ValueType_Enum
}

func GetSchemaFromFeatureService(featureService *model.FeatureService, entityMap map[string]*model.Entity, featureViews []*model.FeatureView, onDemandFeatureViews []*model.OnDemandFeatureView) (*Schema, error) {
	fvs := make(map[string]*model.FeatureView)
	odFvs := make(map[string]*model.OnDemandFeatureView)

	joinKeys := make([]string, 0)
	// All joinkeys in the featureService are put in here
	joinKeysSet := make(map[string]interface{})
	entityJoinKeyToType := make(map[string]types.ValueType_Enum)
	var entities []string
	for _, featureView := range featureViews {
		fvs[featureView.Base.Name] = featureView
		entities = featureView.Entities
	}

	for _, onDemandFeatureView := range onDemandFeatureViews {
		odFvs[onDemandFeatureView.Base.Name] = onDemandFeatureView
	}

	allFeatureTypes := make(map[string]types.ValueType_Enum)
	features := make([]string, 0)
	for _, featureProjection := range featureService.Projections {
		// Create copies of FeatureView that may contains the same *FeatureView but
		// each differentiated by a *FeatureViewProjection
		featureViewName := featureProjection.Name
		if fv, ok := fvs[featureViewName]; ok {
			for _, f := range featureProjection.Features {
				features = append(features, GetFullFeatureName(featureViewName, f.Name))
				allFeatureTypes[GetFullFeatureName(featureViewName, f.Name)] = f.Dtype
			}
			for _, entityName := range fv.Entities {
				entity := entityMap[entityName]
				if joinKeyAlias, ok := featureProjection.JoinKeyMap[entity.JoinKey]; ok {
					joinKeysSet[joinKeyAlias] = nil
				} else {
					joinKeysSet[entity.JoinKey] = nil
				}
			}
		} else if _, ok := odFvs[featureViewName]; ok {
			for _, f := range featureProjection.Features {
				// TODO(kevjumba) check in test here.
				features = append(features, GetFullFeatureName(featureViewName, f.Name))
				allFeatureTypes[GetFullFeatureName(featureViewName, f.Name)] = f.Dtype
			}
		} else {
			return nil, fmt.Errorf("no such feature view found in feature service %s", featureViewName)
		}
	}

	// Only get entities in the current feature service.
	for _, entity := range entities {
		if _, ok := joinKeysSet[entity]; ok {
			joinKeys = append(joinKeys, entityMap[entity].JoinKey)
			entityJoinKeyToType[entityMap[entity].JoinKey] = entityMap[entity].ValueType
		}
	}

	schema := &Schema{
		Entities:      joinKeys,
		Features:      features,
		EntityTypes:   entityJoinKeyToType,
		FeaturesTypes: allFeatureTypes,
	}
	return schema, nil
}

func GetFullFeatureName(featureViewName string, featureName string) string {
	return fmt.Sprintf("%s__%s", featureViewName, featureName)
}

func (s *LoggingService) GetFcos() (map[string]*model.Entity, []*model.FeatureView, []*model.OnDemandFeatureView, error) {
	odfvs, err := s.fs.ListOnDemandFeatureViews()
	if err != nil {
		return nil, nil, nil, err
	}
	fvs, err := s.fs.ListFeatureViews()
	if err != nil {
		return nil, nil, nil, err
	}
	entities, err := s.fs.ListEntities(true)
	if err != nil {
		return nil, nil, nil, err
	}
	entityMap := make(map[string]*model.Entity)
	for _, entity := range entities {
		entityMap[entity.Name] = entity
	}
	return entityMap, fvs, odfvs, nil
}

func (l *LoggingService) GenerateLogs(featureService *model.FeatureService, joinKeyToEntityValues map[string][]*types.Value, features []*serving.GetOnlineFeaturesResponse_FeatureVector, requestData map[string]*types.RepeatedValue, requestId string) error {
	if len(features) <= 0 {
		return nil
	}

	entitySet, featureViews, odfvs, err := l.GetFcos()
	if err != nil {
		return err
	}
	schema, err := GetSchemaFromFeatureService(featureService, entitySet, featureViews, odfvs)

	if err != nil {
		return err
	}

	numFeatures := len(schema.Features)
	// Should be equivalent to how many entities there are(each feature row has (entity) number of features)
	numRows := len(features[0].Values)

	for row_idx := 0; row_idx < numRows; row_idx++ {
		featureValueLogRow := make([]*types.Value, numFeatures)
		featureStatusLogRow := make([]serving.FieldStatus, numFeatures)
		eventTimestampLogRow := make([]*timestamppb.Timestamp, numFeatures)
		for idx := 0; idx < len(features); idx++ {
			featureValueLogRow[idx] = features[idx].Values[row_idx]
			featureStatusLogRow[idx] = features[idx].Statuses[row_idx]
			eventTimestampLogRow[idx] = features[idx].EventTimestamps[row_idx]
		}
		valuesPerEntityRow := make([]*types.Value, 0)
		// ensure that the entity values are in the order that the schema defines which is the order that ListEntities returns the entities
		for _, joinKey := range schema.Entities {
			valuesPerEntityRow = append(valuesPerEntityRow, joinKeyToEntityValues[joinKey][row_idx])
		}
		newLog := Log{
			EntityValue:     valuesPerEntityRow,
			FeatureValues:   featureValueLogRow,
			FeatureStatuses: featureStatusLogRow,
			EventTimestamps: eventTimestampLogRow,
			RequestId:       requestId,
		}
		err := l.EmitLog(&newLog)
		if err != nil {
			return err
		}
	}
	return nil
}
